"""
week3_pipeline
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from airflow.models import Variable
from astro import sql as aql
from astro.table import Table, Metadata
import pandas as pd
import pendulum

import requests
import psycopg2
from datetime import date
import json
import os
import pandas as pd
RUN_DATE = '2023-11-03'
SCHEMA = 'dblash55'
staging_table_name = """dblash55.stock_data_staging"""
production_table_name = """dblash55.production_stock_data"""

@aql.dataframe(task_id="create_staging_and_production_schema")
def create_staging_and_production_schema_func():
    def create_connection():
        connection_url = str(Variable.get('POSTGRES_CONNECTION'))
        conn = psycopg2.connect(connection_url)
        return conn
    
    connection = create_connection()
    create_database_query = f"""CREATE SCHEMA IF NOT EXISTS {SCHEMA}"""
    
    create_query = f"""
            CREATE TABLE IF NOT EXISTS {staging_table_name} (
                date DATE,
                ticker TEXT,
                high REAL,
                low REAL,
                open REAL,
                close REAL,
                volume INTEGER,
                PRIMARY KEY (date, ticker)
            );
    """
    
    create_production_query = f"""
            CREATE TABLE IF NOT EXISTS {production_table_name} (
                date DATE,
                ticker TEXT,
                high REAL,
                low REAL,
                open REAL,
                close REAL,
                volume INTEGER,
                PRIMARY KEY (date, ticker)
            );
    """
    cursor = connection.cursor()
    cursor.execute(create_database_query)
    cursor.execute(create_query)
    cursor.execute(create_production_query)
    cursor.close()
    connection.commit()
    

@aql.dataframe(task_id="write_data_to_staging")
def write_data_to_staging_func():
    def create_connection():
        connection_url = str(Variable.get('POSTGRES_CONNECTION'))
        conn = psycopg2.connect(connection_url)
        return conn
    
    tickers = ['AAPL', 'TSLA']
    df_values = []
    
    insert_query_start = f"INSERT INTO {staging_table_name} VALUES"
    insert_values = []
    for ticker in tickers:
        response = requests.get(f"""https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{RUN_DATE}/{RUN_DATE}?apiKey=pXuH4Tnstzmoq1mg0kqMq4seqOcbOxuw""")
        data = json.loads(response.text)
        if 'results' not in data:
            if data['resultsCount'] == 0:
                print('It is probably the weekend!')
            else:
                raise ValueError("The API hit its rate limit!")
        results = data['results'][0]
        response_values =  {
            'date': RUN_DATE,
            "ticker" : ticker,
            "high": results["h"],
            "low": results["l"],
            "open":  results["o"],
            "close": results["c"],
            "volume": results["v"]
        }
        insert_values.append('(' + ','.join([
                "'" + RUN_DATE + "'",
                "'" + ticker + "'",
                str(results["h"]),
                str(results["l"]),
                str(results["o"]),
                str(results["c"]),
                str(results["v"])
                ]) + ')')
    
        df_values.append(response_values)
    
    connection = create_connection()
    cursor = connection.cursor()
    
    cursor.execute(insert_query_start + ','.join(insert_values))
    cursor.close()
    connection.commit()
    
    return pd.DataFrame(df_values)

@aql.dataframe(task_id="get_date")
def get_date_func():
    return RUN_DATE

@aql.transform(conn_id="week3_my_connection", task_id="audit")
def audit_func(get_date: Table):
    return """
    WITH today AS (
    SELECT * FROM dblash55.stock_data_staging
    WHERE date = DATE({{get_date}})
    ),
    last_week AS (
    SELECT * FROM dblash55.production_stock_data
    WHERE date = DATE({{get_date}}) - INTERVAL '7 days'
    ),
    aggregate AS (
    
    SELECT t.ticker,
            COUNT(1) > 1 as has_duplicates,
            COUNT(CASE WHEN t.close/l.close > 1.1 OR t.close/l.close < 0.9 THEN 1 END) > 0 AS has_big_change
        FROM today t FULL OUTER JOIN last_week l
        ON t.ticker = l.ticker
    GROUP BY t.ticker
    )
    
    SELECT * FROM aggregate WHERE has_duplicates or has_big_change
    """

@aql.dataframe(task_id="audit_decision")
def audit_decision_func(audit: pd.DataFrame):
    audit_result = audit
    
    if len(audit_result) != 0:
        raise ValueError("THE AUDIT HAS FAILED!")

@aql.run_raw_sql(conn_id="week3_my_connection", task_id="publish_data_to_production", results_format="pandas_dataframe")
def publish_data_to_production_func(get_date: Table):
    return """
    INSERT INTO dblash55.production_stock_data
    SELECT * FROM dblash55.stock_data_staging
    WHERE date = DATE({{get_date}});
    
    DELETE FROM dblash55.stock_data_staging
    WHERE date = DATE({{get_date}});
    """

default_args={
    "owner": "deborah.blashill@gmail.com,Open in Cloud IDE",
}

@dag(
    default_args=default_args,
    schedule="0 0 * * *",
    start_date=pendulum.from_format("2023-11-05", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
    owner_links={
        "deborah.blashill@gmail.com": "mailto:deborah.blashill@gmail.com",
        "Open in Cloud IDE": "https://cloud.astronomer.io/clku4tvf8000301kb7wl8bxun/cloud-ide/clolw7ajy00sv01jjdaaz48am/clolw7pit00qe01irs1blf7pz",
    },
)
def week3_pipeline():
    get_date = get_date_func()

    create_staging_and_production_schema = create_staging_and_production_schema_func()

    write_data_to_staging = write_data_to_staging_func()

    audit = audit_func(
        get_date,
    )

    audit_decision = audit_decision_func(
        audit,
    )

    publish_data_to_production = publish_data_to_production_func(
        get_date,
    )

    audit << [write_data_to_staging, get_date]

    audit_decision << audit

    publish_data_to_production << [audit_decision, get_date]

    write_data_to_staging << create_staging_and_production_schema

dag_obj = week3_pipeline()
